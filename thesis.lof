\contentsline {figure}{\numberline {1.1}{\ignorespaces An overview of typical human motion generation approaches. Example images adapted from \blx@tocontentsinit {0}\cite {dummy}.\relax }}{2}{figure.caption.9}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualizing semantic gap between action semantics and raw motion.\relax }}{3}{figure.caption.10}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Illustration and mathematical symbols for various human motion generation process. The virtual character in red (Character A) represents the actor performing an action sequence. The character in blue (Character B) represents the actor performing a reaction sequence. (a) The illustration for reaction generation, when action motion sequence is given. (b) The illustration for interaction generation, given the conditioned signal $C$.\relax }}{4}{figure.caption.11}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces Human avatar interaction in virtual reality.\relax }}{7}{figure.caption.12}%
\contentsline {figure}{\numberline {3.1}{\ignorespaces (Top) Illustrates the gap between two motion modalities, i.e., raw motion and action descriptions. Understanding human motion requires modeling a complex many-to-many mapping function between motion and action spaces. Fuzzy Qualitative Tokens (FQTs) are presented as an intermediate representation to bridge the gap (bottom) comparison of boolean kinematic facts used by previous studies \blx@tocontentsinit {0}\cite {kinematic-phrases, pose_bits, pose_script} with our FQTs. FQTs provide expressive pose geometry and rich semantic information.\relax }}{10}{figure.caption.13}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Method overview: During training, we encode FQTs, motion and text through their respective transformer encoders, together with modal-specific learnable distribution tokens. Each encoder outputs Gaussian distribution parameters, subject to KL losses, from which a latent vector $z$ is sampled. The decoder uses the sampled variable to interpolate, predict, and generate a motion sequence.\relax }}{11}{figure.caption.14}%
\contentsline {figure}{\numberline {4.1}{\ignorespaces Overview of the proposed model (left) DE-CVAE network with two encoders and a decoder. QMTs and atomic action vectors are extracted from action-motion using QMTE and AAE modules, respectively (right) QMTE module, AAE module, and atomic action codebook.\relax }}{13}{figure.caption.15}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces (Top left) Motion sequence (top middle) orientational and positional quantizations (top right) extracted quantized motion tokens (bottom) visual representations for QPT, QPRPT, QPDT, QLAT, QLOT, and QJVT.\relax }}{14}{figure.caption.16}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Visualization of motion generation on SBU dataset for punching class. Skeletons in red represent acting character, while the other colors correspond to the reacting character in various methods. (top to bottom) represent motion sequences for groundtruth, generated by methods \blx@tocontentsinit {0}\cite {gan-reaction-motion}, \blx@tocontentsinit {0}\cite {interaction_transformer}, \blx@tocontentsinit {0}\cite {remos}, \blx@tocontentsinit {0}\cite {interaction-humanoid}, and our results, respectively. (left to right) selective frames during temporal transition.\relax }}{16}{figure.caption.17}%
\contentsfinish 
